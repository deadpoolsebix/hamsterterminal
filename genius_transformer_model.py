"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    GENIUS TRANSFORMER MODEL v1.0                              â•‘
â•‘                    Advanced Deep Learning Price Prediction                    â•‘
â•‘                                                                              â•‘
â•‘  Features:                                                                   â•‘
â•‘  â€¢ Transformer architecture (better than LSTM for long sequences)           â•‘
â•‘  â€¢ Multi-head self-attention mechanism                                      â•‘
â•‘  â€¢ Positional encoding for temporal awareness                               â•‘
â•‘  â€¢ Multiple prediction horizons (1h, 4h, 24h, 7d)                          â•‘
â•‘  â€¢ Ensemble with technical indicators                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, Any, List, Tuple, Optional
from dataclasses import dataclass
import logging
import json
import os

# Try to import TensorFlow
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    TF_AVAILABLE = True
except ImportError:
    TF_AVAILABLE = False
    print("âš ï¸ TensorFlow not installed. Run: pip install tensorflow")

# Try to import PyTorch (alternative)
try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False


@dataclass
class TransformerConfig:
    """Configuration for Transformer model"""
    
    # Model architecture
    d_model: int = 64           # Embedding dimension
    num_heads: int = 4          # Number of attention heads
    num_layers: int = 2         # Number of transformer blocks
    d_ff: int = 128             # Feed-forward dimension
    dropout_rate: float = 0.1   # Dropout rate
    
    # Input/Output
    sequence_length: int = 96   # Input sequence length (96 hours = 4 days)
    num_features: int = 5       # OHLCV features
    prediction_horizons: List[int] = None  # [1, 4, 24, 168] hours
    
    # Training
    batch_size: int = 32
    epochs: int = 50
    learning_rate: float = 0.001
    
    def __post_init__(self):
        if self.prediction_horizons is None:
            self.prediction_horizons = [1, 4, 24, 168]  # 1h, 4h, 24h, 7d


@dataclass
class PredictionResult:
    """Container for prediction results"""
    timestamp: datetime
    current_price: float
    predictions: Dict[str, float]  # horizon -> predicted price
    directions: Dict[str, str]     # horizon -> UP/DOWN
    confidence: Dict[str, float]   # horizon -> confidence
    attention_weights: Optional[np.ndarray] = None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TENSORFLOW TRANSFORMER (Primary Implementation)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class PositionalEncoding(layers.Layer if TF_AVAILABLE else object):
    """
    Sinusoidal positional encoding for temporal awareness
    """
    
    def __init__(self, d_model: int, max_len: int = 5000, **kwargs):
        if not TF_AVAILABLE:
            return
        super().__init__(**kwargs)
        self.d_model = d_model
        self.max_len = max_len
        
        # Create positional encoding matrix
        position = np.arange(max_len)[:, np.newaxis]
        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))
        
        pe = np.zeros((max_len, d_model))
        pe[:, 0::2] = np.sin(position * div_term)
        pe[:, 1::2] = np.cos(position * div_term)
        
        self.pe = tf.constant(pe[np.newaxis, :, :], dtype=tf.float32)
    
    def call(self, x):
        seq_len = tf.shape(x)[1]
        return x + self.pe[:, :seq_len, :]


class TransformerBlock(layers.Layer if TF_AVAILABLE else object):
    """
    Single Transformer block with multi-head attention
    """
    
    def __init__(
        self,
        d_model: int,
        num_heads: int,
        d_ff: int,
        dropout_rate: float = 0.1,
        **kwargs
    ):
        if not TF_AVAILABLE:
            return
        super().__init__(**kwargs)
        
        self.attention = layers.MultiHeadAttention(
            num_heads=num_heads,
            key_dim=d_model // num_heads,
            dropout=dropout_rate
        )
        
        self.ffn = keras.Sequential([
            layers.Dense(d_ff, activation='gelu'),
            layers.Dropout(dropout_rate),
            layers.Dense(d_model)
        ])
        
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(dropout_rate)
        self.dropout2 = layers.Dropout(dropout_rate)
    
    def call(self, x, training=False, return_attention=False):
        # Multi-head self-attention
        attn_output, attn_weights = self.attention(
            x, x, x,
            training=training,
            return_attention_scores=True
        )
        attn_output = self.dropout1(attn_output, training=training)
        x = self.layernorm1(x + attn_output)
        
        # Feed-forward network
        ffn_output = self.ffn(x, training=training)
        ffn_output = self.dropout2(ffn_output, training=training)
        x = self.layernorm2(x + ffn_output)
        
        if return_attention:
            return x, attn_weights
        return x


class GeniusTransformerTF:
    """
    TensorFlow-based Transformer model for price prediction
    """
    
    def __init__(self, config: TransformerConfig = None):
        self.config = config or TransformerConfig()
        self.model = None
        self.scaler_params = {}
        self.is_trained = False
        
        if TF_AVAILABLE:
            self._build_model()
    
    def _build_model(self):
        """Build the Transformer model"""
        
        config = self.config
        
        # Input layer
        inputs = layers.Input(shape=(config.sequence_length, config.num_features))
        
        # Project to d_model dimensions
        x = layers.Dense(config.d_model)(inputs)
        
        # Add positional encoding
        x = PositionalEncoding(config.d_model, config.sequence_length)(x)
        x = layers.Dropout(config.dropout_rate)(x)
        
        # Stack Transformer blocks
        attention_weights = None
        for i in range(config.num_layers):
            if i == config.num_layers - 1:  # Last layer
                x, attention_weights = TransformerBlock(
                    d_model=config.d_model,
                    num_heads=config.num_heads,
                    d_ff=config.d_ff,
                    dropout_rate=config.dropout_rate,
                    name=f'transformer_block_{i}'
                )(x, return_attention=True)
            else:
                x = TransformerBlock(
                    d_model=config.d_model,
                    num_heads=config.num_heads,
                    d_ff=config.d_ff,
                    dropout_rate=config.dropout_rate,
                    name=f'transformer_block_{i}'
                )(x)
        
        # Global average pooling
        x = layers.GlobalAveragePooling1D()(x)
        
        # Output heads for different prediction horizons
        outputs = []
        for horizon in config.prediction_horizons:
            head = layers.Dense(32, activation='relu', name=f'head_{horizon}h_dense')(x)
            head = layers.Dropout(config.dropout_rate)(head)
            output = layers.Dense(1, name=f'pred_{horizon}h')(head)
            outputs.append(output)
        
        self.model = keras.Model(inputs=inputs, outputs=outputs)
        
        # Compile with multi-output loss
        self.model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=config.learning_rate),
            loss=['mse'] * len(config.prediction_horizons),
            metrics=['mae']
        )
    
    def prepare_data(
        self,
        df: pd.DataFrame,
        target_col: str = 'close'
    ) -> Tuple[np.ndarray, List[np.ndarray]]:
        """
        Prepare data for training/prediction
        
        Args:
            df: DataFrame with OHLCV data
            target_col: Target column name
            
        Returns:
            X: Input sequences
            y: List of target arrays for each horizon
        """
        
        config = self.config
        
        # Extract features
        features = ['open', 'high', 'low', 'close', 'volume']
        available_features = [f for f in features if f in df.columns]
        
        if len(available_features) < config.num_features:
            # Generate synthetic features if needed
            if 'close' in df.columns:
                df = df.copy()
                df['open'] = df['close'].shift(1).fillna(df['close'])
                df['high'] = df['close'] * 1.01
                df['low'] = df['close'] * 0.99
                df['volume'] = 1000000
                available_features = features
        
        data = df[available_features].values
        
        # Normalize
        self.scaler_params = {
            'mean': data.mean(axis=0),
            'std': data.std(axis=0) + 1e-8
        }
        
        data_normalized = (data - self.scaler_params['mean']) / self.scaler_params['std']
        
        # Create sequences
        X = []
        y = {h: [] for h in config.prediction_horizons}
        
        max_horizon = max(config.prediction_horizons)
        
        for i in range(config.sequence_length, len(data) - max_horizon):
            X.append(data_normalized[i - config.sequence_length:i])
            
            # Get close price index
            close_idx = available_features.index('close')
            
            for horizon in config.prediction_horizons:
                # Target is the percentage change
                current_price = data[i, close_idx]
                future_price = data[i + horizon, close_idx]
                pct_change = (future_price - current_price) / current_price
                y[horizon].append(pct_change)
        
        X = np.array(X)
        y_arrays = [np.array(y[h]) for h in config.prediction_horizons]
        
        return X, y_arrays
    
    def train(
        self,
        df: pd.DataFrame,
        validation_split: float = 0.2,
        verbose: int = 1
    ) -> Dict[str, Any]:
        """
        Train the Transformer model
        
        Args:
            df: DataFrame with OHLCV data
            validation_split: Fraction for validation
            verbose: Training verbosity
            
        Returns:
            Training history
        """
        
        if not TF_AVAILABLE:
            raise ImportError("TensorFlow required for training")
        
        X, y_arrays = self.prepare_data(df)
        
        # Early stopping
        early_stop = keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True
        )
        
        # Train
        history = self.model.fit(
            X, y_arrays,
            batch_size=self.config.batch_size,
            epochs=self.config.epochs,
            validation_split=validation_split,
            callbacks=[early_stop],
            verbose=verbose
        )
        
        self.is_trained = True
        
        return history.history
    
    def predict(self, df: pd.DataFrame) -> PredictionResult:
        """
        Make predictions using the trained model
        
        Args:
            df: DataFrame with recent OHLCV data
            
        Returns:
            PredictionResult with predictions for all horizons
        """
        
        if not TF_AVAILABLE:
            return self._fallback_predict(df)
        
        if not self.is_trained:
            return self._fallback_predict(df)
        
        # Prepare input
        features = ['open', 'high', 'low', 'close', 'volume']
        available_features = [f for f in features if f in df.columns]
        
        data = df[available_features].values[-self.config.sequence_length:]
        
        # Normalize
        data_normalized = (data - self.scaler_params['mean']) / self.scaler_params['std']
        
        # Predict
        X = data_normalized.reshape(1, self.config.sequence_length, -1)
        predictions = self.model.predict(X, verbose=0)
        
        # Convert predictions to prices
        current_price = df['close'].iloc[-1]
        
        pred_prices = {}
        directions = {}
        confidences = {}
        
        for i, horizon in enumerate(self.config.prediction_horizons):
            pct_change = predictions[i][0][0]
            pred_price = current_price * (1 + pct_change)
            pred_prices[f'{horizon}h'] = float(pred_price)
            directions[f'{horizon}h'] = 'UP' if pct_change > 0 else 'DOWN'
            confidences[f'{horizon}h'] = min(1.0, abs(pct_change) * 10)  # Scale confidence
        
        return PredictionResult(
            timestamp=datetime.now(),
            current_price=float(current_price),
            predictions=pred_prices,
            directions=directions,
            confidence=confidences,
            attention_weights=None
        )
    
    def _fallback_predict(self, df: pd.DataFrame) -> PredictionResult:
        """Fallback prediction using technical analysis"""
        
        current_price = df['close'].iloc[-1]
        
        # Calculate simple momentum
        returns = df['close'].pct_change()
        momentum_1h = returns.iloc[-1] if len(returns) > 0 else 0
        momentum_4h = returns.iloc[-4:].mean() if len(returns) >= 4 else momentum_1h
        momentum_24h = returns.iloc[-24:].mean() if len(returns) >= 24 else momentum_4h
        momentum_168h = returns.iloc[-168:].mean() if len(returns) >= 168 else momentum_24h
        
        # Project prices
        pred_prices = {
            '1h': float(current_price * (1 + momentum_1h * 2)),
            '4h': float(current_price * (1 + momentum_4h * 8)),
            '24h': float(current_price * (1 + momentum_24h * 48)),
            '168h': float(current_price * (1 + momentum_168h * 336))
        }
        
        directions = {k: 'UP' if v > current_price else 'DOWN' for k, v in pred_prices.items()}
        confidences = {k: 0.5 for k in pred_prices.keys()}  # Low confidence for fallback
        
        return PredictionResult(
            timestamp=datetime.now(),
            current_price=float(current_price),
            predictions=pred_prices,
            directions=directions,
            confidence=confidences
        )
    
    def save_model(self, path: str):
        """Save the trained model"""
        if self.model and self.is_trained:
            self.model.save(path)
            
            # Save scaler params
            with open(f"{path}_scaler.json", 'w') as f:
                json.dump({
                    'mean': self.scaler_params['mean'].tolist(),
                    'std': self.scaler_params['std'].tolist()
                }, f)
    
    def load_model(self, path: str):
        """Load a trained model"""
        if TF_AVAILABLE:
            self.model = keras.models.load_model(path)
            
            # Load scaler params
            with open(f"{path}_scaler.json", 'r') as f:
                params = json.load(f)
                self.scaler_params = {
                    'mean': np.array(params['mean']),
                    'std': np.array(params['std'])
                }
            
            self.is_trained = True


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PYTORCH TRANSFORMER (Alternative Implementation)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if TORCH_AVAILABLE:
    class TransformerBlockPyTorch(nn.Module):
        """PyTorch Transformer block"""
        
        def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
            super().__init__()
            self.attention = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)
            self.ffn = nn.Sequential(
                nn.Linear(d_model, d_ff),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(d_ff, d_model)
            )
            self.norm1 = nn.LayerNorm(d_model)
            self.norm2 = nn.LayerNorm(d_model)
            self.dropout = nn.Dropout(dropout)
        
        def forward(self, x):
            attn_out, _ = self.attention(x, x, x)
            x = self.norm1(x + self.dropout(attn_out))
            ffn_out = self.ffn(x)
            x = self.norm2(x + self.dropout(ffn_out))
            return x
    
    class GeniusTransformerPyTorch(nn.Module):
        """PyTorch-based Transformer for price prediction"""
        
        def __init__(self, config: TransformerConfig = None):
            super().__init__()
            self.config = config or TransformerConfig()
            
            self.input_projection = nn.Linear(config.num_features, config.d_model)
            
            # Positional encoding
            self.register_buffer('pe', self._create_positional_encoding())
            
            # Transformer layers
            self.transformer_layers = nn.ModuleList([
                TransformerBlockPyTorch(
                    config.d_model, config.num_heads, config.d_ff, config.dropout_rate
                )
                for _ in range(config.num_layers)
            ])
            
            # Output heads
            self.output_heads = nn.ModuleList([
                nn.Sequential(
                    nn.Linear(config.d_model, 32),
                    nn.ReLU(),
                    nn.Dropout(config.dropout_rate),
                    nn.Linear(32, 1)
                )
                for _ in config.prediction_horizons
            ])
        
        def _create_positional_encoding(self):
            """Create sinusoidal positional encoding"""
            config = self.config
            position = torch.arange(config.sequence_length).unsqueeze(1)
            div_term = torch.exp(torch.arange(0, config.d_model, 2) * -(np.log(10000.0) / config.d_model))
            
            pe = torch.zeros(1, config.sequence_length, config.d_model)
            pe[0, :, 0::2] = torch.sin(position * div_term)
            pe[0, :, 1::2] = torch.cos(position * div_term)
            return pe
        
        def forward(self, x):
            # Project input
            x = self.input_projection(x)
            
            # Add positional encoding
            x = x + self.pe[:, :x.size(1), :]
            
            # Apply transformer layers
            for layer in self.transformer_layers:
                x = layer(x)
            
            # Global average pooling
            x = x.mean(dim=1)
            
            # Output predictions
            outputs = [head(x) for head in self.output_heads]
            return outputs


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GENIUS TRANSFORMER PREDICTOR (Main Interface)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class GeniusTransformerPredictor:
    """
    Main interface for Transformer-based price prediction
    
    Features:
    - Automatic backend selection (TensorFlow/PyTorch)
    - Multi-horizon predictions
    - Ensemble with technical indicators
    - Confidence scoring
    """
    
    def __init__(self, config: TransformerConfig = None, backend: str = 'auto'):
        self.config = config or TransformerConfig()
        self.backend = self._select_backend(backend)
        self.model = self._create_model()
        
        self.logger = logging.getLogger('GeniusTransformerPredictor')
        
        # Prediction history
        self.prediction_history: List[PredictionResult] = []
    
    def _select_backend(self, backend: str) -> str:
        """Select the best available backend"""
        
        if backend == 'auto':
            if TF_AVAILABLE:
                return 'tensorflow'
            elif TORCH_AVAILABLE:
                return 'pytorch'
            else:
                return 'fallback'
        
        return backend
    
    def _create_model(self):
        """Create the model based on backend"""
        
        if self.backend == 'tensorflow' and TF_AVAILABLE:
            return GeniusTransformerTF(self.config)
        elif self.backend == 'pytorch' and TORCH_AVAILABLE:
            return GeniusTransformerPyTorch(self.config)
        else:
            return None
    
    def train(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """Train the model"""
        
        if self.backend == 'tensorflow' and self.model:
            return self.model.train(df, **kwargs)
        else:
            self.logger.warning("Training not available for current backend")
            return {}
    
    def predict(self, df: pd.DataFrame) -> PredictionResult:
        """Make predictions"""
        
        if self.model and hasattr(self.model, 'predict'):
            result = self.model.predict(df)
        else:
            result = self._quick_predict(df)
        
        self.prediction_history.append(result)
        return result
    
    def _quick_predict(self, df: pd.DataFrame) -> PredictionResult:
        """Quick prediction using momentum and mean reversion"""
        
        current_price = df['close'].iloc[-1]
        
        # Calculate indicators
        returns = df['close'].pct_change().dropna()
        
        # Momentum (trend following)
        momentum = returns.ewm(span=12).mean().iloc[-1] if len(returns) >= 12 else 0
        
        # Mean reversion signal
        sma_20 = df['close'].rolling(20).mean().iloc[-1] if len(df) >= 20 else current_price
        mean_reversion = (sma_20 - current_price) / current_price
        
        # Volatility
        volatility = returns.std() * np.sqrt(24) if len(returns) >= 24 else 0.02
        
        # Combined signal
        signal = momentum * 0.6 + mean_reversion * 0.4
        
        # Predictions with increasing uncertainty
        predictions = {}
        directions = {}
        confidences = {}
        
        for horizon, weight in [(1, 1), (4, 2), (24, 5), (168, 15)]:
            pred_change = signal * weight * np.sqrt(horizon)
            # Add volatility-based uncertainty
            uncertainty = volatility * np.sqrt(horizon / 24)
            
            pred_price = current_price * (1 + pred_change)
            predictions[f'{horizon}h'] = float(pred_price)
            directions[f'{horizon}h'] = 'UP' if pred_change > 0 else 'DOWN'
            confidences[f'{horizon}h'] = max(0.1, min(0.9, 0.7 - uncertainty))
        
        return PredictionResult(
            timestamp=datetime.now(),
            current_price=float(current_price),
            predictions=predictions,
            directions=directions,
            confidence=confidences
        )
    
    def get_trading_signal(self, result: PredictionResult, horizon: str = '4h') -> Dict[str, Any]:
        """
        Convert prediction to trading signal
        
        Args:
            result: PredictionResult from predict()
            horizon: Which prediction horizon to use
            
        Returns:
            Signal dict for Genius Engine integration
        """
        
        direction = result.directions.get(horizon, 'NEUTRAL')
        confidence = result.confidence.get(horizon, 0)
        pred_price = result.predictions.get(horizon, result.current_price)
        
        pct_change = (pred_price - result.current_price) / result.current_price * 100
        
        # Minimum confidence and change threshold
        if confidence < 0.4 or abs(pct_change) < 0.5:
            return {
                'active': False,
                'points': 0,
                'direction': 'NEUTRAL',
                'reason': 'Low confidence or weak prediction'
            }
        
        # Calculate points (max 15 for transformer)
        base_points = 15
        points = int(base_points * confidence * min(1.0, abs(pct_change) / 3))
        
        return {
            'active': True,
            'points': points,
            'direction': 'LONG' if direction == 'UP' else 'SHORT',
            'confidence': confidence,
            'predicted_price': pred_price,
            'predicted_change_pct': pct_change,
            'horizon': horizon
        }
    
    def format_prediction(self, result: PredictionResult) -> str:
        """Format prediction for display"""
        
        lines = []
        lines.append("=" * 50)
        lines.append("ğŸ¤– TRANSFORMER PREDICTION")
        lines.append("=" * 50)
        lines.append(f"Current Price: ${result.current_price:,.2f}")
        lines.append(f"Timestamp: {result.timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append("")
        lines.append("Predictions:")
        
        for horizon in ['1h', '4h', '24h', '168h']:
            if horizon in result.predictions:
                pred = result.predictions[horizon]
                direction = result.directions[horizon]
                conf = result.confidence[horizon]
                change = (pred - result.current_price) / result.current_price * 100
                
                emoji = "ğŸ“ˆ" if direction == 'UP' else "ğŸ“‰"
                lines.append(
                    f"  {horizon:>4}: ${pred:,.2f} {emoji} ({change:+.2f}%) "
                    f"[Conf: {conf:.1%}]"
                )
        
        return "\n".join(lines)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DEMO / TEST
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def demo():
    """Demo Transformer model"""
    
    print("=" * 70)
    print("ğŸ¤– GENIUS TRANSFORMER MODEL v1.0 - DEMO")
    print("=" * 70)
    
    # Check available backends
    print(f"\nğŸ“¦ Backend Availability:")
    print(f"   TensorFlow: {'âœ… Available' if TF_AVAILABLE else 'âŒ Not installed'}")
    print(f"   PyTorch:    {'âœ… Available' if TORCH_AVAILABLE else 'âŒ Not installed'}")
    
    # Create predictor
    predictor = GeniusTransformerPredictor()
    print(f"\n   Using backend: {predictor.backend.upper()}")
    
    # Create synthetic data
    print("\nğŸ“Š Creating synthetic BTC data...")
    np.random.seed(42)
    
    n_points = 500
    base_price = 97500
    
    # Random walk with trend
    returns = np.random.normal(0.0002, 0.02, n_points)
    prices = [base_price]
    for r in returns:
        prices.append(prices[-1] * (1 + r))
    
    df = pd.DataFrame({
        'close': prices,
        'open': [p * 0.999 for p in prices],
        'high': [p * 1.01 for p in prices],
        'low': [p * 0.99 for p in prices],
        'volume': np.random.uniform(1e9, 2e9, n_points + 1)
    })
    
    print(f"   Generated {len(df)} hourly candles")
    print(f"   Price range: ${df['close'].min():,.2f} - ${df['close'].max():,.2f}")
    
    # Make prediction
    print("\nğŸ”® Making predictions...")
    result = predictor.predict(df)
    
    print(predictor.format_prediction(result))
    
    # Get trading signal
    print("\nğŸ¯ TRADING SIGNAL (4h horizon):")
    print("-" * 50)
    signal = predictor.get_trading_signal(result, '4h')
    
    print(f"   Active:     {signal['active']}")
    print(f"   Direction:  {signal.get('direction', 'N/A')}")
    print(f"   Points:     {signal['points']}")
    if signal['active']:
        print(f"   Predicted:  ${signal['predicted_price']:,.2f}")
        print(f"   Change:     {signal['predicted_change_pct']:+.2f}%")
    
    # Configuration display
    print("\nâš™ï¸ MODEL CONFIGURATION:")
    print("-" * 50)
    config = predictor.config
    print(f"   d_model:     {config.d_model}")
    print(f"   num_heads:   {config.num_heads}")
    print(f"   num_layers:  {config.num_layers}")
    print(f"   d_ff:        {config.d_ff}")
    print(f"   seq_length:  {config.sequence_length}")
    print(f"   horizons:    {config.prediction_horizons}")
    
    print("\n" + "=" * 70)
    print("âœ… Transformer model ready!")
    if not TF_AVAILABLE and not TORCH_AVAILABLE:
        print("   âš ï¸ Install TensorFlow or PyTorch for full functionality:")
        print("   pip install tensorflow")
    print("=" * 70)
    
    return True


if __name__ == "__main__":
    demo()
